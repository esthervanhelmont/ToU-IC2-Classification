{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f70a95",
   "metadata": {},
   "source": [
    "## Load cleaned data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc0ed79",
   "metadata": {},
   "source": [
    "### Imports and reproducibility\n",
    "\n",
    "Here we bring in the basic tools we need.  \n",
    "\n",
    "- `os` and `sqlite3` help us work with files and a small database  \n",
    "- `numpy` and `pandas` help us handle numbers and tables  \n",
    "- `matplotlib.pyplot` lets us make simple plots  \n",
    "- `sklearn` is the machine learning toolbox with models and metrics  \n",
    "\n",
    "We also set something called **reproducibility**.  \n",
    "That means if we run the notebook today or tomorrow we always get the same result.  \n",
    "\n",
    "Why do we need this  \n",
    "Some models in machine learning pick random numbers inside.  \n",
    "If we do not control the random numbers we may get slightly different results each run.  \n",
    "That makes it hard to compare experiments.  \n",
    "\n",
    "The line `RANDOM_STATE = 42` sets a seed for the random number generator.  \n",
    "So each time the random choices are made they follow the same path.  \n",
    "This makes our experiments repeatable and fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee75178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix, RocCurveDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f793f",
   "metadata": {},
   "source": [
    "### Load the cleaned data\n",
    "\n",
    "We start by loading the cleaned dataset from a CSV file.  \n",
    "CSV is just a simple text file with rows and columns like a spreadsheet.  \n",
    "\n",
    "Then we also save this same data into a small database called **SQLite**.  \n",
    "Why do we do this  \n",
    "- A database makes it easy to query or filter later  \n",
    "- It keeps the same workflow as in preprocessing  \n",
    "- It shows you how data can live in different formats (CSV and database)  \n",
    "\n",
    "After saving we read the data back from the database into a pandas DataFrame.  \n",
    "This way we check that everything works and that the data looks the same.  \n",
    "\n",
    "In the end we print the shape of the table (rows × columns) and show the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7ad1606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (99991, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>smoking_history</th>\n",
       "      <th>bmi</th>\n",
       "      <th>hba1c_level</th>\n",
       "      <th>blood_glucose_level</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age_group</th>\n",
       "      <th>bmi_category</th>\n",
       "      <th>risk_score</th>\n",
       "      <th>age_bmi_interaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>never</td>\n",
       "      <td>25.19</td>\n",
       "      <td>6.6</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>senior</td>\n",
       "      <td>overweight</td>\n",
       "      <td>1</td>\n",
       "      <td>2015.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>27.32</td>\n",
       "      <td>6.6</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>senior</td>\n",
       "      <td>overweight</td>\n",
       "      <td>0</td>\n",
       "      <td>1475.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>never</td>\n",
       "      <td>27.32</td>\n",
       "      <td>5.7</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>young</td>\n",
       "      <td>overweight</td>\n",
       "      <td>0</td>\n",
       "      <td>764.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>current</td>\n",
       "      <td>23.45</td>\n",
       "      <td>5.0</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>middle</td>\n",
       "      <td>normal</td>\n",
       "      <td>1</td>\n",
       "      <td>844.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>current</td>\n",
       "      <td>20.14</td>\n",
       "      <td>4.8</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>senior</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "      <td>1530.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender   age  hypertension  heart_disease smoking_history    bmi  \\\n",
       "0  Female  80.0             0              1           never  25.19   \n",
       "1  Female  54.0             0              0         unknown  27.32   \n",
       "2    Male  28.0             0              0           never  27.32   \n",
       "3  Female  36.0             0              0         current  23.45   \n",
       "4    Male  76.0             1              1         current  20.14   \n",
       "\n",
       "   hba1c_level  blood_glucose_level  diabetes age_group bmi_category  \\\n",
       "0          6.6                  140         0    senior   overweight   \n",
       "1          6.6                   80         0    senior   overweight   \n",
       "2          5.7                  158         0     young   overweight   \n",
       "3          5.0                  155         0    middle       normal   \n",
       "4          4.8                  155         0    senior       normal   \n",
       "\n",
       "   risk_score  age_bmi_interaction  \n",
       "0           1              2015.20  \n",
       "1           0              1475.28  \n",
       "2           0               764.96  \n",
       "3           1               844.20  \n",
       "4           3              1530.64  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = '../data/processed/diabetes_clean.csv'\n",
    "db_path = '../data/processed/diabetes.db'\n",
    "table_name = 'diabetes_clean'\n",
    "\n",
    "# read csv\n",
    "df_csv = pd.read_csv(csv_path)\n",
    "\n",
    "# write to sqlite\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(db_path)\n",
    "df_csv.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "# read back from sqlite\n",
    "df = pd.read_sql(f'SELECT * FROM {table_name}', conn)\n",
    "conn.close()\n",
    "\n",
    "print('Data shape:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a2405",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We imported all the tools we need for machine learning and fixed the random seed so results will always be the same each run  \n",
    "This gives us reproducibility  \n",
    "\n",
    "We loaded the cleaned dataset from a CSV file and also stored it in a small SQLite database  \n",
    "\n",
    "With this we are sure the data is ready and stable for the next steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffe467",
   "metadata": {},
   "source": [
    "## Define target and features\n",
    "\n",
    "We now set the target column to be  **`diabetes`**  \n",
    "This is the column we want the model to predict (0 = no diabetes, 1 = diabetes)  \n",
    "\n",
    "We call this column `y`  \n",
    "All the other columns are called `X` → these are the features the model will use to make a prediction  \n",
    "\n",
    "So in short  \n",
    "- `y` = the answer we want to predict  \n",
    "- `X` = the clues the model can look at\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "466a7d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column: diabetes\n"
     ]
    }
   ],
   "source": [
    "target_col = 'diabetes'\n",
    "y = df[target_col]\n",
    "X = df.drop(columns=[target_col])\n",
    "print(\"Target column:\", target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77484005",
   "metadata": {},
   "source": [
    "**Conclusion**  \n",
    "We clearly told the notebook that our target is `diabetes`  \n",
    "Now we have a clean split between features (`X`) and target (`y`)  \n",
    "This makes it ready for the next steps like checking leakage and training models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196a842",
   "metadata": {},
   "source": [
    "## Feature leakage check\n",
    "\n",
    "### Build two feature sets to check leakage\n",
    "\n",
    "The biggest challenge so far is deciding whether certain features could cause data leakage. \n",
    "\n",
    "For example, Insulin levels are often only available during or after diagnostic processes, which may make them less realistic for prediction. (so we already remove them)\n",
    "\n",
    "Glucose is another concern because it is very highly correlated with the target, and might make the model perform unrealistically well.\n",
    "\n",
    "On the one hand, glucose is the most important clinical indicator for diabetes. Physicians base the diagnosis primarily on blood glucose levels (for example, fasting glucose or HbA1c).\n",
    "\n",
    "On the other hand, glucose can also be measured as part of a standard blood test, so it is not always only available when someone is already suspected of having diabetes.\n",
    "I’m going to test the models with and without  \n",
    "\n",
    "So we make two versions of our dataset  \n",
    "- Variant A keeps all numeric features (including glucose)  \n",
    "- Variant B removes glucose and insulin so we can test if results are more realistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a5d2ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant A shape (99991, 8) Variant B shape (99991, 7)\n"
     ]
    }
   ],
   "source": [
    "# split features and target\n",
    "y = df[target_col]\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# keep only numeric for a simple baseline\n",
    "X_num = X.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# Variant A: all numeric features (possible leakage included)\n",
    "X_A = X_num.copy()\n",
    "\n",
    "# Variant B: remove only blood_glucose_level to reduce leakage\n",
    "X_B = X_num.drop(columns=['blood_glucose_level'], errors='ignore').copy()\n",
    "\n",
    "print('Variant A shape', X_A.shape, 'Variant B shape', X_B.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13291d11",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Variant A uses all features while Variant B excludes blood_glucose_level  \n",
    "This way we can later compare results and see how much impact this feature has on the model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb03c98d",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "### Train-test split for both variants\n",
    "\n",
    "\n",
    "We split the data into training and testing sets  \n",
    "- Training data is used to teach the model  \n",
    "- Testing data is kept aside to check how well the model performs on unseen data  \n",
    "\n",
    "We do this for both Variant A (with blood_glucose_level) and Variant B (without it)  \n",
    "\n",
    "We use  \n",
    "- `test_size=0.2` → 20 percent test, 80 percent train  \n",
    "- `random_state` → for reproducibility  \n",
    "- `stratify=y` → to keep the same balance of 0 and 1 in train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ecb4d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant A - train: (79992, 8) test: (19999, 8)\n",
      "Variant B - train: (79992, 7) test: (19999, 7)\n"
     ]
    }
   ],
   "source": [
    "# split data for Variant A\n",
    "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(\n",
    "    X_A, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# split data for Variant B\n",
    "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(\n",
    "    X_B, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Variant A - train:\", X_train_A.shape, \"test:\", X_test_A.shape)\n",
    "print(\"Variant B - train:\", X_train_B.shape, \"test:\", X_test_B.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5c4b9",
   "metadata": {},
   "source": [
    "**Conclusion**  \n",
    "We now have two clean splits  \n",
    "- Variant A with all features  \n",
    "- Variant B without blood_glucose_level  \n",
    "\n",
    "Both have 80 percent of the rows for training and 20 percent for testing  \n",
    "This makes the setup ready for model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd58dcc",
   "metadata": {},
   "source": [
    "## Modeltraining Variant A (probably leakage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e110d",
   "metadata": {},
   "source": [
    "\n",
    "We start with very simple models so we keep things clear  \n",
    "We fill empty values with the median  \n",
    "We scale the features only for logistic regression  \n",
    "We train three models: \n",
    "- logistic regression  \n",
    "- decision tree  \n",
    "- random forest  \n",
    "\n",
    "Then we check simple scores  \n",
    "accuracy precision recall f1 and roc auc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03dcbe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic A {'accuracy': 0.9573978698934946, 'precision': 0.9820627802690582, 'recall': 0.43887775551102204, 'f1': 0.6066481994459834, 'roc_auc': np.float64(0.9583984926578556)}\n",
      "Tree A {'accuracy': 0.9304965248262413, 'precision': 0.5342729019859065, 'recall': 0.5571142284569138, 'f1': 0.5454545454545454, 'roc_auc': np.float64(0.76118727564306)}\n",
      "RF A {'accuracy': 0.9526976348817441, 'precision': 0.8184971098265896, 'recall': 0.4729458917835671, 'f1': 0.5994919559695173, 'roc_auc': np.float64(0.9406687478657817)}\n"
     ]
    }
   ],
   "source": [
    "# fill missing values with median\n",
    "X_train_A_imputed = X_train_A.fillna(X_train_A.median(numeric_only=True))\n",
    "X_test_A_imputed = X_test_A.fillna(X_train_A.median(numeric_only=True))\n",
    "\n",
    "# scale only for logistic regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_A = StandardScaler()\n",
    "X_train_A_scaled = scaler_A.fit_transform(X_train_A_imputed)\n",
    "X_test_A_scaled = scaler_A.transform(X_test_A_imputed)\n",
    "\n",
    "# make models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "log_reg_A = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "tree_A = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "rf_A = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE)\n",
    "\n",
    "# fit models\n",
    "log_reg_A.fit(X_train_A_scaled, y_train_A)\n",
    "tree_A.fit(X_train_A_imputed, y_train_A)\n",
    "rf_A.fit(X_train_A_imputed, y_train_A)\n",
    "\n",
    "# helper to compute simple metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def eval_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    proba_ok = hasattr(model, \"predict_proba\")\n",
    "    score = None\n",
    "    if proba_ok:\n",
    "        score = model.predict_proba(X_test)[:, 1]\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_test, score) if score is not None else None\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# evaluate\n",
    "metrics_log_A = eval_model(log_reg_A, X_test_A_scaled, y_test_A)\n",
    "metrics_tree_A = eval_model(tree_A, X_test_A_imputed, y_test_A)\n",
    "metrics_rf_A = eval_model(rf_A, X_test_A_imputed, y_test_A)\n",
    "\n",
    "print(\"Logistic A\", metrics_log_A)\n",
    "print(\"Tree A\", metrics_tree_A)\n",
    "print(\"RF A\", metrics_rf_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a83b9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4a0c76e",
   "metadata": {},
   "source": [
    "## Modeltraining Variant B (without possible leakage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88779750",
   "metadata": {},
   "source": [
    "Now we train the same three models on Variant B  \n",
    "Variant B does not include the column blood_glucose_level  \n",
    "This gives us a more honest test because we removed the possible leakage  \n",
    "\n",
    "We again  \n",
    "- fill missing values with the median  \n",
    "- scale only for logistic regression  \n",
    "- train logistic regression, decision tree and random forest  \n",
    "\n",
    "Then we check the same scores  \n",
    "accuracy precision recall f1 and roc auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e449f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic B {'accuracy': 0.9573978698934946, 'precision': 0.9835082458770614, 'recall': 0.43820975283901137, 'f1': 0.6062846580406654, 'roc_auc': np.float64(0.9583931311619744)}\n",
      "Tree B {'accuracy': 0.9363968198409921, 'precision': 0.5791695988740324, 'recall': 0.5497661990647963, 'f1': 0.5640849897189856, 'roc_auc': np.float64(0.7782430244411281)}\n",
      "RF B {'accuracy': 0.9505475273763688, 'precision': 0.7679324894514767, 'recall': 0.4863059452237809, 'f1': 0.5955010224948876, 'roc_auc': np.float64(0.9316041371829523)}\n"
     ]
    }
   ],
   "source": [
    "# fill missing values with median\n",
    "X_train_B_imputed = X_train_B.fillna(X_train_B.median(numeric_only=True))\n",
    "X_test_B_imputed = X_test_B.fillna(X_train_B.median(numeric_only=True))\n",
    "\n",
    "# scale only for logistic regression\n",
    "scaler_B = StandardScaler()\n",
    "X_train_B_scaled = scaler_B.fit_transform(X_train_B_imputed)\n",
    "X_test_B_scaled = scaler_B.transform(X_test_B_imputed)\n",
    "\n",
    "# make models\n",
    "log_reg_B = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "tree_B = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "rf_B = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE)\n",
    "\n",
    "# fit models\n",
    "log_reg_B.fit(X_train_B_scaled, y_train_B)\n",
    "tree_B.fit(X_train_B_imputed, y_train_B)\n",
    "rf_B.fit(X_train_B_imputed, y_train_B)\n",
    "\n",
    "# evaluate\n",
    "metrics_log_B = eval_model(log_reg_B, X_test_B_scaled, y_test_B)\n",
    "metrics_tree_B = eval_model(tree_B, X_test_B_imputed, y_test_B)\n",
    "metrics_rf_B = eval_model(rf_B, X_test_B_imputed, y_test_B)\n",
    "\n",
    "print(\"Logistic B\", metrics_log_B)\n",
    "print(\"Tree B\", metrics_tree_B)\n",
    "print(\"RF B\", metrics_rf_B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a9e1f",
   "metadata": {},
   "source": [
    "## Compare results of Variant A and Variant B\n",
    "\n",
    "| Model              | Variant | Accuracy | Precision | Recall | F1   | ROC-AUC |\n",
    "|--------------------|---------|----------|-----------|--------|------|---------|\n",
    "| Logistic Regression | A       | 0.9574   | 0.9821    | 0.4389 | 0.6066 | 0.9584 |\n",
    "| Decision Tree       | A       | 0.9305   | 0.5343    | 0.5571 | 0.5455 | 0.7612 |\n",
    "| Random Forest       | A       | 0.9527   | 0.8185    | 0.4729 | 0.5995 | 0.9407 |\n",
    "| Logistic Regression | B       | 0.9574   | 0.9835    | 0.4382 | 0.6063 | 0.9584 |\n",
    "| Decision Tree       | B       | 0.9364   | 0.5792    | 0.5498 | 0.5641 | 0.7782 |\n",
    "| Random Forest       | B       | 0.9505   | 0.7679    | 0.4863 | 0.5955 | 0.9316 |\n",
    "\n",
    "**Conclusion**  \n",
    "- Logistic Regression performs almost the same in both Variant A and B  \n",
    "  → very high accuracy and precision, but recall is low (it misses many true cases of diabetes)  \n",
    "- Decision Tree improves slightly in Variant B (better precision and f1, higher roc-auc)  \n",
    "- Random Forest is strong in both, with very similar results  \n",
    "- Removing `blood_glucose_level` did **not drastically change** the performance, so the model is not fully dependent on that single feature  \n",
    "\n",
    "In simple words  \n",
    "The models do not collapse without blood_glucose_level  \n",
    "This means they can still learn useful patterns from other features  \n",
    "But recall is low for Logistic Regression and Random Forest → they catch fewer true positives  \n",
    "Decision Tree balances precision and recall better but with lower overall accuracy  \n",
    "\n",
    "### Conclusion\n",
    "Looking at the results  \n",
    "\n",
    "- **Logistic Regression** has the highest accuracy and precision  \n",
    "  But recall is low → it misses many people who actually have diabetes  \n",
    "- **Decision Tree** has the best balance between precision and recall  \n",
    "  But overall accuracy is lower  \n",
    "- **Random Forest** sits in the middle → good accuracy, better recall than Logistic Regression, and solid roc-auc  \n",
    "\n",
    "If the goal is to **catch as many true diabetes cases as possible** (high recall), then **Random Forest** is the best option  \n",
    "If the goal is to **avoid false alarms** (high precision), then **Logistic Regression** is a strong choice  \n",
    "For a balanced trade-off, **Random Forest** is the safest pick  \n",
    "\n",
    "### Final Choice \n",
    "We will continue with **Random Forest on Variant B**  \n",
    "This model gives the most reliable results  \n",
    "It combines good accuracy with better recall than Logistic Regression  \n",
    "and it does not depend too much on blood_glucose_level  \n",
    "This makes it a fair and realistic choice for our final pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f67866",
   "metadata": {},
   "source": [
    "## Best model + pipeline\n",
    "\n",
    "### Finalize and save the Variant B pipeline\n",
    "\n",
    "We finish with Random Forest on Variant B  \n",
    "We fill missing values with the median  \n",
    "We train on the full Variant B dataset so the model sees all rows  \n",
    "We save the model with joblib together with the feature list  \n",
    "We also make a simple feature importance chart so we see which clues matter most\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d545ec0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/diabetes_clf_pipeline.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# save pipeline parts\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_B\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimputer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedian by column\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvariant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mB without blood_glucose_level\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/mnt/data/diabetes_clf_pipeline.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved to /mnt/data/diabetes_clf_pipeline.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# quick feature importance visual\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/numpy_pickle.py:599\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol)\u001b[0m\n\u001b[1;32m    597\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    600\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/diabetes_clf_pipeline.pkl'"
     ]
    }
   ],
   "source": [
    "# prepare full Variant B data\n",
    "X_full_B = X_B.fillna(X_B.median(numeric_only=True))\n",
    "y_full = y\n",
    "\n",
    "# train final random forest on all Variant B rows\n",
    "rf_final = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE)\n",
    "rf_final.fit(X_full_B, y_full)\n",
    "\n",
    "# save pipeline parts\n",
    "import joblib\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": rf_final,\n",
    "        \"features\": X_B.columns.tolist(),\n",
    "        \"imputer\": \"median by column\",\n",
    "        \"variant\": \"B without blood_glucose_level\"\n",
    "    },\n",
    "    \"/mnt/data/diabetes_clf_pipeline.pkl\"\n",
    ")\n",
    "\n",
    "print(\"saved to /mnt/data/diabetes_clf_pipeline.pkl\")\n",
    "\n",
    "# quick feature importance visual\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "importances = pd.Series(rf_final.feature_importances_, index=X_B.columns).sort_values(ascending=True)\n",
    "topk = min(10, len(importances))\n",
    "imp_top = importances.tail(topk)\n",
    "\n",
    "plt.figure()\n",
    "imp_top.plot(kind=\"barh\")\n",
    "plt.title(\"feature importance random forest variant B\")\n",
    "plt.xlabel(\"importance\")\n",
    "plt.ylabel(\"feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0eb4e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9559d9b",
   "metadata": {},
   "source": [
    "## Save pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
